{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNq0wqJ1bge7kpMvVQkrHsT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidhtang/facial-recognition-with-open-cv-and-cnn/blob/main/Copy_of_facial_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "import cv2\n",
        "from keras.models import model_from_json\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XxOHU682lsfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rAOLKrfVpfiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUip3U_Llzv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "data = {}\n",
        "with open('/content/facial_expressions/data/legend.csv') as f:\n",
        "  reader = csv.reader(f)\n",
        "  next(reader)\n",
        "  for row in reader:\n",
        "    key = row[2].lower()\n",
        "    if key in data:\n",
        "      data[key].append(row[1])\n",
        "    else:\n",
        "      data[key] = [row[1]]"
      ],
      "metadata": {
        "id": "5GGxxAoExJ0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/muxspace/facial_expressions.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy635j0mlztT",
        "outputId": "b180a196-19e4-40bc-c245-2ba5389e4435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'facial_expressions' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_list = list(data.keys())\n",
        "emotion_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3BXrkpwlphB",
        "outputId": "c05d6085-ad72-453a-ea13-ad498fae7bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['anger',\n",
              " 'surprise',\n",
              " 'disgust',\n",
              " 'fear',\n",
              " 'neutral',\n",
              " 'happiness',\n",
              " 'sadness',\n",
              " 'contempt']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for each emotion we will create a subdirectory\n",
        "from shutil import copyfile\n",
        "split_size = 0.8\n",
        "\n",
        "for emotion, images in data.items():\n",
        "  train_size = int(split_size*len(images))\n",
        "  train_images = images[:train_size]\n",
        "  test_images = images[train_size:]\n",
        "  for image in train_images:\n",
        "    source = os.path.join('/content/facial_expressions/images', image)\n",
        "    dest = os.path.join('/content/master_data/training', emotion, image)\n",
        "    copyfile(source, dest)\n",
        "  for image in test_images:\n",
        "    source = os.path.join('/content/facial_expressions/images', image)\n",
        "    dest = os.path.join('/content/master_data/testing', emotion, image)\n",
        "    copyfile(source, dest)"
      ],
      "metadata": {
        "id": "hiVJaOTExJ9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "105318bb-796c-4a4e-cdf4-06d3459fbedc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1d8f68cc2cdc>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/facial_expressions/images'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/master_data/training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten"
      ],
      "metadata": {
        "id": "bb7D5yg1xKDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    Conv2D(16, (3, 3), activation='relu', input_shape=(100, 100, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(8, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.01),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhYlzNCT5IDC",
        "outputId": "198a07c0-4916-42dc-ba61-70411aa5aa37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 98, 98, 16)        448       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 49, 49, 16)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 47, 47, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 23, 23, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 21, 21, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 10, 10, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6400)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               3277312   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 4104      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3305000 (12.61 MB)\n",
            "Trainable params: 3305000 (12.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/content/master_data/training'\n",
        "test_dir = '/content/master_data/testing'\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1.0/255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                                                    train_dir,\n",
        "                                                    target_size = (100, 100),\n",
        "                                                    class_mode = 'categorical',\n",
        "                                                    batch_size = 128\n",
        "                                                  )\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1.0/255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "                                                    test_dir,\n",
        "                                                    target_size = (100, 100),\n",
        "                                                    class_mode = 'categorical',\n",
        "                                                    batch_size = 128\n",
        "                                                  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-lANeXR5IF4",
        "outputId": "af168fbb-497a-4662-8046-bbbf7a2ad308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10941 images belonging to 8 classes.\n",
            "Found 2742 images belonging to 8 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_acc', patience = 2, min_delta=0.01)"
      ],
      "metadata": {
        "id": "4SlYiUy15IJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_generator(train_generator,\n",
        "                    epochs = 10,\n",
        "                    verbose = 1,\n",
        "                    validation_data = test_generator,\n",
        "                    callbacks = [es])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqF7kojd5INA",
        "outputId": "dcb64af2-f474-44b5-91e3-e701d4a57f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-a67ff5996604>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(train_generator,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86/86 [==============================] - ETA: 0s - loss: 1.6193 - accuracy: 0.4726"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r86/86 [==============================] - 21s 200ms/step - loss: 1.6193 - accuracy: 0.4726 - val_loss: 1.0527 - val_accuracy: 0.5011\n",
            "Epoch 2/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 1.0436 - accuracy: 0.4923"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r86/86 [==============================] - 15s 179ms/step - loss: 1.0436 - accuracy: 0.4923 - val_loss: 1.0482 - val_accuracy: 0.5011\n",
            "Epoch 3/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 1.0438 - accuracy: 0.4971"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r86/86 [==============================] - 18s 208ms/step - loss: 1.0438 - accuracy: 0.4971 - val_loss: 1.0464 - val_accuracy: 0.4158\n",
            "Epoch 4/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 1.0453 - accuracy: 0.4826"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r86/86 [==============================] - 15s 175ms/step - loss: 1.0453 - accuracy: 0.4826 - val_loss: 1.0434 - val_accuracy: 0.5011\n",
            "Epoch 5/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 1.0407 - accuracy: 0.5018"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r86/86 [==============================] - 15s 175ms/step - loss: 1.0407 - accuracy: 0.5018 - val_loss: 1.0423 - val_accuracy: 0.5011\n",
            "Epoch 6/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 1.0393 - accuracy: 0.5018"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r86/86 [==============================] - 15s 174ms/step - loss: 1.0393 - accuracy: 0.5018 - val_loss: 1.0426 - val_accuracy: 0.5011\n",
            "Epoch 7/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 1.0403 - accuracy: 0.4980"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r86/86 [==============================] - 18s 210ms/step - loss: 1.0403 - accuracy: 0.4980 - val_loss: 1.0446 - val_accuracy: 0.5011\n",
            "Epoch 8/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 1.0395 - accuracy: 0.5018"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r86/86 [==============================] - 15s 178ms/step - loss: 1.0395 - accuracy: 0.5018 - val_loss: 1.0421 - val_accuracy: 0.5011\n",
            "Epoch 9/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 1.0399 - accuracy: 0.5018"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r86/86 [==============================] - 15s 171ms/step - loss: 1.0399 - accuracy: 0.5018 - val_loss: 1.0440 - val_accuracy: 0.5011\n",
            "Epoch 10/10\n",
            "86/86 [==============================] - ETA: 0s - loss: 1.0411 - accuracy: 0.4935"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r86/86 [==============================] - 17s 202ms/step - loss: 1.0411 - accuracy: 0.4935 - val_loss: 1.0476 - val_accuracy: 0.5011\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c0e3ff16770>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oIT470IBkSS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QqmHIY70kSVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N96ZtMPBkSZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6GlCIZ7lkUJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZltY9_QkUNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# save trained model weight in .h5 file\n",
        "model.save_weights('model.h5')"
      ],
      "metadata": {
        "id": "Ng3dD_EL5IQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\",\n",
        "                3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}"
      ],
      "metadata": {
        "id": "xAVmmtO4lR45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model.load_weights(\"model.h5\")"
      ],
      "metadata": {
        "id": "oJmqPIECkV-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Assuming the model and emotion_dict are defined somewhere\n",
        "# model = ...\n",
        "# emotion_dict = ...\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "if not cap.isOpened():\n",
        "    print(\"Error opening video stream or file\")\n",
        "    exit()  # Exit if the video capture fails\n",
        "\n",
        "while True:\n",
        "    # Find haar cascade to draw bounding box around face\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:  # Check if a frame was successfully read\n",
        "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
        "        break\n",
        "\n",
        "    frame = cv2.resize(frame, (1280, 720))\n",
        "    if not ret:\n",
        "        print(ret)\n",
        "        break\n",
        "\n",
        "    # Create a face detector\n",
        "    face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces available on camera\n",
        "    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    # Take each face available on the camera and preprocess it\n",
        "    for (x, y, w, h) in num_faces:\n",
        "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
        "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
        "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
        "\n",
        "        # Predict emotion\n",
        "        emotion_prediction = model.predict(cropped_img)\n",
        "        maxindex = int(np.argmax(emotion_prediction))\n",
        "\n",
        "        # Put text of emotion on the frame\n",
        "        cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "    cv2.imshow('Emotion Detection', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkmrN5YdkWLo",
        "outputId": "089272b4-d300-487f-fa0d-8d4cb8c0fc80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error opening video stream or file\n",
            "Can't receive frame (stream end?). Exiting ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-UrGFFdkWPL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}