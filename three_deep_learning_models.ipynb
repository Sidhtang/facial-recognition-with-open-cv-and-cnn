{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMbfvc4pt4kHrkQRu7ta1n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidhtang/facial-recognition-with-open-cv-and-cnn/blob/main/three_deep_learning_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w8gFKdQc5Epc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics colorthief deep_sort_realtime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Mefo1jXDOVrg",
        "outputId": "18d41672-df30-4851-8e4f-69233f15c542"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.59-py3-none-any.whl.metadata (35 kB)\n",
            "Collecting colorthief\n",
            "  Downloading colorthief-0.2.1-py2.py3-none-any.whl.metadata (816 bytes)\n",
            "Collecting deep_sort_realtime\n",
            "  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.13-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.59-py3-none-any.whl (906 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.8/906.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorthief-0.2.1-py2.py3-none-any.whl (6.1 kB)\n",
            "Downloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.13-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: colorthief, deep_sort_realtime, ultralytics-thop, ultralytics\n",
            "Successfully installed colorthief-0.2.1 deep_sort_realtime-1.3.2 ultralytics-8.3.59 ultralytics-thop-2.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# another optimised version of the  code with different prerainrd transformer with temporal smoothing\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.cluster import KMeans\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import efficientnet_v2_l, resnet152\n",
        "from PIL import Image\n",
        "import csv\n",
        "from colorthief import ColorThief\n",
        "import io\n",
        "import albumentations as A\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up GPU device with mixed precision training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if device == 'cuda':\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "class EfficientNetFPN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        # Load EfficientNetV2 as backbone but remove the classifier\n",
        "        self.backbone = efficientnet_v2_l(pretrained=True)\n",
        "\n",
        "        # Store the feature extraction points from EfficientNet\n",
        "        # These correspond to different scales of features\n",
        "        self.features = {}\n",
        "\n",
        "        # Remove the classifier and pooling layers as we'll use FPN instead\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
        "\n",
        "        # Define FPN layers\n",
        "        # Numbers correspond to EfficientNetV2's channel dimensions\n",
        "        self.lateral_convs = nn.ModuleList([\n",
        "            nn.Conv2d(1280, 256, 1),  # For the highest level features\n",
        "            nn.Conv2d(640, 256, 1),   # For mid-level features\n",
        "            nn.Conv2d(320, 256, 1)    # For lower-level features\n",
        "        ])\n",
        "\n",
        "        # Smooth the merged features\n",
        "        self.smooth_convs = nn.ModuleList([\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.Conv2d(256, 256, 3, padding=1)\n",
        "        ])\n",
        "\n",
        "        # Final classifier using multi-scale features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 3, 512),  # Concatenate features from all scales\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights properly\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features at different scales from EfficientNet\n",
        "        features = []\n",
        "        for i, layer in enumerate(self.backbone):\n",
        "            x = layer(x)\n",
        "            if i in {5, 6, 7}:  # Collect features from different scales\n",
        "                features.append(x)\n",
        "\n",
        "        # Build top-down pathway (FPN)\n",
        "        laterals = []\n",
        "        for feature, lateral_conv in zip(features[::-1], self.lateral_convs):\n",
        "            # Convert features to common channel dimension\n",
        "            lat = lateral_conv(feature)\n",
        "            laterals.append(lat)\n",
        "\n",
        "        # Process from top to bottom, upsampling and adding features\n",
        "        fpn_features = [laterals[0]]\n",
        "        for i in range(len(laterals) - 1):\n",
        "            # Upsample higher level features\n",
        "            upsampled = F.interpolate(fpn_features[-1],\n",
        "                                    size=laterals[i + 1].shape[-2:],\n",
        "                                    mode='nearest')\n",
        "            # Add features from lateral connection\n",
        "            merged = upsampled + laterals[i + 1]\n",
        "            # Apply smoothing\n",
        "            smoothed = self.smooth_convs[i](merged)\n",
        "            fpn_features.append(smoothed)\n",
        "\n",
        "        # Combine multi-scale features\n",
        "        multi_scale_features = []\n",
        "        for fpn_feature in fpn_features:\n",
        "            pooled = F.adaptive_avg_pool2d(fpn_feature, 1)\n",
        "            multi_scale_features.append(pooled)\n",
        "\n",
        "        # Concatenate and classify\n",
        "        combined = torch.cat(multi_scale_features, dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "class EnhancedColorClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedColorClassifier, self).__init__()\n",
        "        # Using ResNet152 for better color feature extraction\n",
        "        self.features = resnet152(pretrained=True)\n",
        "        num_ftrs = self.features.fc.in_features\n",
        "\n",
        "        # Enhanced classifier head with attention mechanism\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.BatchNorm1d(num_ftrs),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, len(COLOR_CLASSES))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.cuda.amp.autocast():\n",
        "            features = self.features.conv1(x)\n",
        "            features = self.features.bn1(features)\n",
        "            features = self.features.relu(features)\n",
        "            features = self.features.maxpool(features)\n",
        "\n",
        "            features = self.features.layer1(features)\n",
        "            features = self.features.layer2(features)\n",
        "            features = self.features.layer3(features)\n",
        "            features = self.features.layer4(features)\n",
        "\n",
        "            features = self.features.avgpool(features)\n",
        "            features = torch.flatten(features, 1)\n",
        "\n",
        "            # Apply attention mechanism\n",
        "            attention_weights = self.attention(features)\n",
        "            attention_weights = torch.sigmoid(attention_weights)\n",
        "            attended_features = features * attention_weights\n",
        "\n",
        "            return self.classifier(attended_features)\n",
        "\n",
        "# Enhanced constants with more detailed categories\n",
        "VEHICLE_SUBTYPES = {\n",
        "    'car': ['sedan', 'suv', 'hatchback', 'wagon', 'coupe', 'sports_car', 'luxury', 'compact',\n",
        "            'convertible', 'crossover', 'electric_vehicle'],\n",
        "    'truck': ['pickup', 'semi', 'delivery', 'dump_truck', 'box_truck', 'flatbed', 'tanker',\n",
        "              'concrete_mixer', 'car_carrier'],\n",
        "    'bus': ['city_bus', 'coach', 'mini_bus', 'school_bus', 'articulated_bus', 'double_decker',\n",
        "            'shuttle_bus'],\n",
        "    'van': ['passenger_van', 'cargo_van', 'minivan', 'camper_van', 'panel_van', 'refrigerated_van',\n",
        "            'step_van'],\n",
        "    'two_wheeler': ['motorcycle', 'scooter', 'bicycle', 'electric_bike', 'moped', 'sport_bike',\n",
        "                   'cruiser', 'touring_bike']\n",
        "}\n",
        "\n",
        "COLOR_CLASSES = [\n",
        "    'black', 'white', 'gray', 'silver', 'red', 'blue', 'green', 'yellow',\n",
        "    'brown', 'orange', 'purple', 'gold', 'beige', 'burgundy', 'navy',\n",
        "    'teal', 'bronze', 'copper', 'champagne'\n",
        "]\n",
        "\n",
        "def get_enhanced_transforms():\n",
        "    \"\"\"\n",
        "    Create enhanced image preprocessing pipeline with augmentations\n",
        "    \"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((384, 384)),  # Larger input size for better detail capture\n",
        "        transforms.RandomHorizontalFlip(p=0.3),\n",
        "        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "def classify_vehicle_details(frame, box, class_name, vehicle_classifier, confidence_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Enhanced vehicle classification with confidence thresholding and error handling\n",
        "    \"\"\"\n",
        "    try:\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        vehicle_img = frame[y1:y2, x1:x2]\n",
        "\n",
        "        # Check if image patch is valid\n",
        "        if vehicle_img.size == 0 or vehicle_img.shape[0] == 0 or vehicle_img.shape[1] == 0:\n",
        "            return None\n",
        "\n",
        "        # Convert to PIL and apply enhanced preprocessing\n",
        "        img_pil = Image.fromarray(cv2.cvtColor(vehicle_img, cv2.COLOR_BGR2RGB))\n",
        "        transform = get_enhanced_transforms()\n",
        "        input_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = vehicle_classifier(input_tensor)\n",
        "                probabilities = torch.softmax(outputs, dim=1)\n",
        "                conf, predicted = probabilities.max(1)\n",
        "\n",
        "                # Only classify if confidence exceeds threshold\n",
        "                if conf.item() < confidence_threshold:\n",
        "                    return None\n",
        "\n",
        "        subtype = list(VEHICLE_SUBTYPES[class_name])[predicted.item() % len(VEHICLE_SUBTYPES[class_name])]\n",
        "\n",
        "        # Enhanced size classification using relative dimensions\n",
        "        area = (x2 - x1) * (y2 - y1)\n",
        "        frame_area = frame.shape[0] * frame.shape[1]\n",
        "        area_ratio = area / frame_area\n",
        "\n",
        "        size = 'small' if area_ratio < 0.05 else \\\n",
        "               'medium' if area_ratio < 0.15 else 'large'\n",
        "\n",
        "        # Enhanced body style and purpose classification\n",
        "        body_style = 'commercial' if subtype in ['semi', 'delivery', 'dump_truck', 'box_truck', 'tanker',\n",
        "                                               'concrete_mixer', 'car_carrier', 'refrigerated_van'] else \\\n",
        "                    'passenger' if subtype in ['city_bus', 'school_bus', 'coach', 'shuttle_bus'] else 'standard'\n",
        "\n",
        "        purpose = 'commercial' if body_style == 'commercial' else \\\n",
        "                 'public' if body_style == 'passenger' else 'personal'\n",
        "\n",
        "        return {\n",
        "            'subtype': subtype,\n",
        "            'size': size,\n",
        "            'body_style': body_style,\n",
        "            'purpose': purpose,\n",
        "            'confidence': conf.item()\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Vehicle classification error: {e}\")\n",
        "        return None\n",
        "\n",
        "def setup_models():\n",
        "    \"\"\"\n",
        "    Initialize enhanced models with optimized parameters\n",
        "    \"\"\"\n",
        "    # Initialize YOLOv8x with enhanced parameters\n",
        "    yolo_model = YOLO('yolov8x.pt')\n",
        "    yolo_model.conf = 0.35  # Lower confidence threshold for higher recall\n",
        "    yolo_model.iou = 0.65   # Higher IoU threshold for better precision\n",
        "    yolo_model.to(device)\n",
        "\n",
        "    # Initialize enhanced classifiers\n",
        "    vehicle_classifier = EnhancedVehicleClassifier().to(device)\n",
        "    vehicle_classifier.eval()\n",
        "\n",
        "    color_classifier = EnhancedColorClassifier().to(device)\n",
        "    color_classifier.eval()\n",
        "\n",
        "    # Initialize DeepSort with optimized parameters\n",
        "    tracker = DeepSort(\n",
        "        max_age=45,            # Increased to handle longer occlusions\n",
        "        n_init=4,              # More frames required for track initialization\n",
        "        nms_max_overlap=0.85,  # Higher NMS threshold for better tracking\n",
        "        max_cosine_distance=0.25,  # Stricter matching threshold\n",
        "        nn_budget=150,         # Larger budget for better tracking history\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'yolo': yolo_model,\n",
        "        'vehicle_classifier': vehicle_classifier,\n",
        "        'color_classifier': color_classifier,\n",
        "        'tracker': tracker,\n",
        "        'device': device\n",
        "    }\n",
        "\n",
        "def detect_color(frame, box, color_classifier, confidence_threshold=0.6):\n",
        "    \"\"\"\n",
        "    Enhanced color detection with multi-method approach and confidence thresholding\n",
        "    \"\"\"\n",
        "    try:\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        vehicle_img = frame[y1:y2, x1:x2]\n",
        "\n",
        "        if vehicle_img.size == 0:\n",
        "            return None\n",
        "\n",
        "        # Convert to PIL Image\n",
        "        img_pil = Image.fromarray(cv2.cvtColor(vehicle_img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # Enhanced color detection using multiple methods\n",
        "        results = {'colors': [], 'confidences': []}\n",
        "\n",
        "        # Method 1: ML Classification\n",
        "        transform = get_enhanced_transforms()\n",
        "        input_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = color_classifier(input_tensor)\n",
        "                probabilities = torch.softmax(outputs, dim=1)\n",
        "                conf, predicted = probabilities.max(1)\n",
        "\n",
        "                if conf.item() >= confidence_threshold:\n",
        "                    results['colors'].append(COLOR_CLASSES[predicted.item()])\n",
        "                    results['confidences'].append(conf.item())\n",
        "\n",
        "        # Method 2: ColorThief analysis\n",
        "        img_byte_arr = io.BytesIO()\n",
        "        img_pil.save(img_byte_arr, format='PNG')\n",
        "        img_byte_arr.seek(0)\n",
        "\n",
        "        color_thief = ColorThief(img_byte_arr)\n",
        "        dominant_color = color_thief.get_color(quality=1)\n",
        "        palette = color_thief.get_palette(color_count=3, quality=1)\n",
        "\n",
        "        # Convert RGB values to color names using nearest neighbor\n",
        "        for rgb in [dominant_color] + palette:\n",
        "            color_name = get_closest_color(rgb)\n",
        "            if color_name not in results['colors']:\n",
        "                results['colors'].append(color_name)\n",
        "                results['confidences'].append(0.8)  # Default confidence for color matching\n",
        "\n",
        "        # Return most confident color\n",
        "        if results['colors']:\n",
        "            max_conf_idx = np.argmax(results['confidences'])\n",
        "            return {\n",
        "                'color': results['colors'][max_conf_idx],\n",
        "                'confidence': results['confidences'][max_conf_idx],\n",
        "                'secondary_colors': results['colors'][:3]  # Top 3 detected colors\n",
        "            }\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Color detection error: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_closest_color(rgb):\n",
        "    \"\"\"\n",
        "    Convert RGB value to the closest named color using an extended color mapping.\n",
        "    \"\"\"\n",
        "    color_map = {\n",
        "        'black': (0, 0, 0),\n",
        "        'white': (255, 255, 255),\n",
        "        'gray': (128, 128, 128),\n",
        "        'silver': (192, 192, 192),\n",
        "        'red': (255, 0, 0),\n",
        "        'blue': (0, 0, 255),\n",
        "        'green': (0, 255, 0),\n",
        "        'yellow': (255, 255, 0),\n",
        "        'brown': (165, 42, 42),\n",
        "        'orange': (255, 165, 0),\n",
        "        'purple': (128, 0, 128),\n",
        "        'gold': (255, 215, 0),\n",
        "        'beige': (245, 245, 220),\n",
        "        'burgundy': (128, 0, 32),\n",
        "        'navy': (0, 0, 128),\n",
        "        'teal': (0, 128, 128),\n",
        "        'bronze': (205, 127, 50),\n",
        "        'copper': (184, 115, 51),\n",
        "        'champagne': (247, 231, 206),\n",
        "        'pink': (255, 192, 203),\n",
        "        'cyan': (0, 255, 255),\n",
        "        'lime': (191, 255, 0),\n",
        "        'magenta': (255, 0, 255),\n",
        "        'olive': (128, 128, 0),\n",
        "        'peach': (255, 229, 180),\n",
        "    }\n",
        "\n",
        "    min_distance = float('inf')\n",
        "    closest_color = None\n",
        "\n",
        "    for color_name, color_rgb in color_map.items():\n",
        "        distance = sum((a - b) ** 2 for a, b in zip(rgb, color_rgb))\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            closest_color = color_name\n",
        "\n",
        "    return closest_color\n",
        "\n",
        "\n",
        "    for color_name, color_rgb in color_map.items():\n",
        "        distance = sum((a - b) ** 2 for a, b in zip(rgb, color_rgb))\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            closest_color = color_name\n",
        "\n",
        "    return closest_color\n",
        "\n",
        "def process_video(video_path, output_path, models, data_output_path):\n",
        "    \"\"\"\n",
        "    Enhanced video processing with multi-frame analysis and temporal smoothing\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Could not open video: {video_path}\")\n",
        "\n",
        "    # Video writer setup\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
        "\n",
        "    data = []\n",
        "    frame_count = 0\n",
        "    temporal_buffer = {}  # Store recent detections for temporal smoothing\n",
        "\n",
        "    try:\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "            # Process every frame for maximum accuracy\n",
        "            detection_list = []\n",
        "\n",
        "            # Run YOLO detection with test-time augmentation\n",
        "            results = models['yolo'](frame, augment=True)  # Enable test-time augmentation\n",
        "\n",
        "            # Process YOLO detections\n",
        "            for box, conf, cls in zip(results[0].boxes.xyxy, results[0].boxes.conf, results[0].boxes.cls):\n",
        "                x1, y1, x2, y2 = map(int, box.tolist())\n",
        "                detection = ([x1, y1, x2 - x1, y2 - y1], conf.item(), int(cls.item()))\n",
        "                detection_list.append(detection)\n",
        "\n",
        "            # Update tracks with enhanced DeepSort\n",
        "            tracks = models['tracker'].update_tracks(detection_list, frame=frame)\n",
        "\n",
        "            # Process each track with temporal smoothing\n",
        "            for track in tracks:\n",
        "                if not track.is_confirmed():\n",
        "                    continue\n",
        "\n",
        "                track_id = track.track_id\n",
        "                ltwh = track\n",
        "                to_ltwh = track.to_ltwh()\n",
        "                box = [int(to_ltwh[0]), int(to_ltwh[1]),\n",
        "                      int(to_ltwh[0] + to_ltwh[2]), int(to_ltwh[1] + to_ltwh[3])]\n",
        "\n",
        "                cls_id = track.get_det_class()\n",
        "                if cls_id is None:\n",
        "                    continue\n",
        "\n",
        "                class_name = models['yolo'].names[cls_id]\n",
        "\n",
        "                if class_name in VEHICLE_SUBTYPES:\n",
        "                    # Initialize temporal buffer for this track if needed\n",
        "                    if track_id not in temporal_buffer:\n",
        "                        temporal_buffer[track_id] = {\n",
        "                            'vehicle_details': [],\n",
        "                            'color_info': [],\n",
        "                            'frame_history': []\n",
        "                        }\n",
        "\n",
        "                    # Get vehicle details\n",
        "                    vehicle_details = classify_vehicle_details(\n",
        "                        frame, box, class_name,\n",
        "                        models['vehicle_classifier']\n",
        "                    )\n",
        "\n",
        "                    # Get color information\n",
        "                    color_info = detect_color(\n",
        "                        frame, box,\n",
        "                        models['color_classifier']\n",
        "                    )\n",
        "\n",
        "                    # Update temporal buffer\n",
        "                    if vehicle_details and color_info:\n",
        "                        temporal_buffer[track_id]['vehicle_details'].append(vehicle_details)\n",
        "                        temporal_buffer[track_id]['color_info'].append(color_info)\n",
        "                        temporal_buffer[track_id]['frame_history'].append(frame_count)\n",
        "\n",
        "                        # Keep only recent history (last 5 frames)\n",
        "                        max_history = 5\n",
        "                        if len(temporal_buffer[track_id]['vehicle_details']) > max_history:\n",
        "                            temporal_buffer[track_id]['vehicle_details'] = temporal_buffer[track_id]['vehicle_details'][-max_history:]\n",
        "                            temporal_buffer[track_id]['color_info'] = temporal_buffer[track_id]['color_info'][-max_history:]\n",
        "                            temporal_buffer[track_id]['frame_history'] = temporal_buffer[track_id]['frame_history'][-max_history:]\n",
        "\n",
        "                        # Get smoothed predictions using temporal buffer\n",
        "                        smoothed_details = get_smoothed_predictions(temporal_buffer[track_id])\n",
        "\n",
        "                        # Create detailed label with confidence scores\n",
        "                        label_parts = [\n",
        "                            f\"{smoothed_details['subtype']} ({smoothed_details['size']})\",\n",
        "                            f\"#{track_id}\",\n",
        "                            f\"{smoothed_details['color']}\",\n",
        "                            f\"{smoothed_details['confidence']:.2f}\"\n",
        "                        ]\n",
        "                        label = \" \".join(label_parts)\n",
        "\n",
        "                        # Calculate dynamic color based on confidence\n",
        "                        confidence = smoothed_details['confidence']\n",
        "                        bbox_color = (\n",
        "                            int(255 * (1 - confidence)),  # Red component\n",
        "                            int(255 * confidence),        # Green component\n",
        "                            0                            # Blue component\n",
        "                        )\n",
        "\n",
        "                        # Enhanced visualization\n",
        "                        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), bbox_color, 2)\n",
        "\n",
        "                        # Add background to text for better visibility\n",
        "                        text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
        "                        cv2.rectangle(frame,\n",
        "                                    (box[0], box[1] - 25),\n",
        "                                    (box[0] + text_size[0], box[1]),\n",
        "                                    bbox_color, -1)\n",
        "                        cv2.putText(frame, label, (box[0], box[1] - 10),\n",
        "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "                        # Store detection data with enhanced information\n",
        "                        data.append({\n",
        "                            'Frame': frame_count,\n",
        "                            'Track_ID': track_id,\n",
        "                            'Main_Type': class_name,\n",
        "                            'Subtype': smoothed_details['subtype'],\n",
        "                            'Size': smoothed_details['size'],\n",
        "                            'Body_Style': smoothed_details['body_style'],\n",
        "                            'Purpose': smoothed_details['purpose'],\n",
        "                            'Primary_Color': smoothed_details['color'],\n",
        "                            'Secondary_Colors': smoothed_details['secondary_colors'],\n",
        "                            'Detection_Confidence': track.get_det_conf() or 0.0,\n",
        "                            'Classification_Confidence': smoothed_details['confidence'],\n",
        "                            'Box_Coordinates': box,\n",
        "                            'Temporal_Confidence': len(temporal_buffer[track_id]['vehicle_details']) / max_history\n",
        "                        })\n",
        "\n",
        "            # Clean up old tracks from temporal buffer\n",
        "            current_track_ids = {track.track_id for track in tracks if track.is_confirmed()}\n",
        "            temporal_buffer = {k: v for k, v in temporal_buffer.items() if k in current_track_ids}\n",
        "\n",
        "            # Add frame counter and processing statistics\n",
        "            cv2.putText(frame, f\"Frame: {frame_count}\", (10, 30),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "            out.write(frame)\n",
        "\n",
        "    finally:\n",
        "        # Release resources\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "        # Save detection data with enhanced error handling\n",
        "        if data:\n",
        "            try:\n",
        "                with open(data_output_path, 'w', newline='') as f:\n",
        "                    writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
        "                    writer.writeheader()\n",
        "                    writer.writerows(data)\n",
        "                print(f\"Successfully saved detection data to {data_output_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving detection data: {e}\")\n",
        "                # Attempt to save to alternative location\n",
        "                backup_path = \"backup_\" + data_output_path\n",
        "                with open(backup_path, 'w', newline='') as f:\n",
        "                    writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
        "                    writer.writeheader()\n",
        "                    writer.writerows(data)\n",
        "                print(f\"Saved backup detection data to {backup_path}\")\n",
        "\n",
        "def get_smoothed_predictions(track_buffer):\n",
        "    \"\"\"\n",
        "    Calculate smoothed predictions using temporal buffer data\n",
        "    \"\"\"\n",
        "    # Get recent vehicle details\n",
        "    recent_details = track_buffer['vehicle_details']\n",
        "    recent_colors = track_buffer['color_info']\n",
        "\n",
        "    if not recent_details or not recent_colors:\n",
        "        return None\n",
        "\n",
        "    # Count occurrences of each prediction\n",
        "    subtype_counts = Counter(d['subtype'] for d in recent_details)\n",
        "    size_counts = Counter(d['size'] for d in recent_details)\n",
        "    body_style_counts = Counter(d['body_style'] for d in recent_details)\n",
        "    purpose_counts = Counter(d['purpose'] for d in recent_details)\n",
        "    color_counts = Counter(c['color'] for c in recent_colors)\n",
        "\n",
        "    # Calculate average confidence\n",
        "    avg_confidence = np.mean([d['confidence'] for d in recent_details])\n",
        "\n",
        "    # Get secondary colors from recent detections\n",
        "    all_secondary_colors = []\n",
        "    for color_info in recent_colors:\n",
        "        if 'secondary_colors' in color_info:\n",
        "            all_secondary_colors.extend(color_info['secondary_colors'])\n",
        "    secondary_colors = [color for color, count in Counter(all_secondary_colors).most_common(3)]\n",
        "\n",
        "    # Return smoothed predictions\n",
        "    return {\n",
        "        'subtype': subtype_counts.most_common(1)[0][0],\n",
        "        'size': size_counts.most_common(1)[0][0],\n",
        "        'body_style': body_style_counts.most_common(1)[0][0],\n",
        "        'purpose': purpose_counts.most_common(1)[0][0],\n",
        "        'color': color_counts.most_common(1)[0][0],\n",
        "        'secondary_colors': secondary_colors,\n",
        "        'confidence': avg_confidence\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function with enhanced error handling and logging\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define paths\n",
        "        video_path = \"/content/trimmed version of the input video.mp4\"\n",
        "        output_path = \"output_video.mp4\"\n",
        "        data_output_path = \"vehicle_tracking_data.csv\"\n",
        "\n",
        "        print(\"Initializing models...\")\n",
        "        models = setup_models()\n",
        "        print(\"Models initialized successfully\")\n",
        "\n",
        "        print(\"Starting video processing...\")\n",
        "        process_video(video_path, output_path, models, data_output_path)\n",
        "        print(\"Video processing completed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {e}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "eI-omNt1U4rs",
        "outputId": "c289ae56-7399-40d6-8e00-9e3282a5aaf0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Initializing models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_v2_l-59c71312.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_l-59c71312.pth\n",
            "100%|██████████| 455M/455M [00:08<00:00, 54.6MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
            "100%|██████████| 230M/230M [00:02<00:00, 107MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models initialized successfully\n",
            "Starting video processing...\n",
            "Error in main execution: Could not open video: /content/trimmed version of the input video.mp4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Could not open video: /content/trimmed version of the input video.mp4",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7c781aa2ffef>\u001b[0m in \u001b[0;36m<cell line: 636>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-7c781aa2ffef>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting video processing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mprocess_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_output_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Video processing completed successfully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-7c781aa2ffef>\u001b[0m in \u001b[0;36mprocess_video\u001b[0;34m(video_path, output_path, models, data_output_path)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not open video: {video_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# Video writer setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Could not open video: /content/trimmed version of the input video.mp4"
          ]
        }
      ]
    }
  ]
}